{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.feedback import GroundTruthAgreement\n",
    "from trulens.providers.openai import OpenAI\n",
    "from trulens.core import Feedback\n",
    "from typing import List, Dict\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First, create your ground truth dataset\n",
    "ground_truths = [\n",
    "    {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"expected_response\": \"The capital of France is Paris.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Who wrote Romeo and Juliet?\",\n",
    "        \"expected_response\": \"William Shakespeare wrote Romeo and Juliet.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 2. Initialize the GroundTruthAgreement with your provider\n",
    "ground_truth = GroundTruthAgreement(\n",
    "    ground_truths,\n",
    "    provider=OpenAI(api_key = os.getenv(\"OPENAI_API_KEY\"))  # Or your preferred provider\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In agreement_measure, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In agreement_measure, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In bert_score, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In bert_score, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In bleu, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In bleu, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In rouge, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In rouge, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "# 3. Create feedback functions using different evaluation methods\n",
    "\n",
    "# Using semantic agreement (GPT-based evaluation)\n",
    "f_agreement = Feedback(ground_truth.agreement_measure).on_input_output()\n",
    "\n",
    "# Using BERT Score for semantic similarity\n",
    "f_bert = Feedback(ground_truth.bert_score).on_input_output()\n",
    "\n",
    "# Using BLEU score for token overlap\n",
    "f_bleu = Feedback(ground_truth.bleu).on_input_output()\n",
    "\n",
    "# Using ROUGE score\n",
    "f_rouge = Feedback(ground_truth.rouge).on_input_output()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. For evaluating retrieved contexts specifically\n",
    "def evaluate_retrieval(\n",
    "    query: str,\n",
    "    retrieved_chunks: List[str],\n",
    "    relevance_scores: List[float] = None,\n",
    "    k: int = None\n",
    "):\n",
    "    # Calculate various retrieval metrics\n",
    "    ndcg = ground_truth.ndcg_at_k(query, retrieved_chunks, relevance_scores, k)\n",
    "    precision = ground_truth.precision_at_k(query, retrieved_chunks, relevance_scores, k)\n",
    "    recall = ground_truth.recall_at_k(query, retrieved_chunks, relevance_scores, k)\n",
    "    mrr = ground_truth.mrr(query, retrieved_chunks, relevance_scores)\n",
    "    hit_rate = ground_truth.ir_hit_rate(query, retrieved_chunks, k)\n",
    "    \n",
    "    return {\n",
    "        \"ndcg@k\": ndcg,\n",
    "        \"precision@k\": precision,\n",
    "        \"recall@k\": recall,\n",
    "        \"mrr\": mrr,\n",
    "        \"hit_rate@k\": hit_rate\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Use with your RAG application\n",
    "with tru.recorder() as recording:\n",
    "    # Your RAG query execution here\n",
    "    response = rag_app(query)\n",
    "    \n",
    "    # Get feedback scores\n",
    "    agreement_score = f_agreement(query, response)\n",
    "    bert_score = f_bert(query, response)\n",
    "    bleu_score = f_bleu(query, response)\n",
    "    rouge_score = f_rouge(query, response)\n",
    "    \n",
    "    # If you want to evaluate retrieval specifically\n",
    "    retrieval_metrics = evaluate_retrieval(\n",
    "        query=query,\n",
    "        retrieved_chunks=rag_app.get_retrieved_chunks(),  # Your retrieval function\n",
    "        k=5  # Evaluate top-5 results\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
